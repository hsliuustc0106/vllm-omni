# vLLM-omni Default Configuration

# General settings
general:
  name: "vllm-omni"
  version: "0.1.0"
  debug: false
  log_level: "INFO"

# Model settings
model:
  # Supported model types
  supported_types: ["ar", "diffusion", "multimodal"]
  
  # Default model configurations
  default_ar_model: "qwen2.5-7b"
  default_diffusion_model: "qwen-image"
  
  # Model loading settings
  device: "auto"  # auto, cpu, cuda, mps
  dtype: "auto"   # auto, float16, bfloat16, float32
  max_model_len: 32768

# Engine settings
engines:
  # AR Engine (imported from vLLM)
  ar_engine:
    enabled: true
    max_batch_size: 256
    max_num_seqs: 256
    
  # Diffusion Engine
  diffusion_engine:
    enabled: true
    max_batch_size: 32
    max_num_seqs: 32
    num_inference_steps: 50
    guidance_scale: 7.5

# Processing settings
processing:
  # Input processing
  input:
    max_image_size: [1024, 1024]
    max_audio_length: 30.0  # seconds
    supported_image_formats: ["jpg", "jpeg", "png", "bmp", "webp"]
    supported_audio_formats: ["wav", "mp3", "flac", "m4a"]
    
  # Output processing
  output:
    max_output_length: 4096
    streaming: true
    return_hidden_states: false

# API settings
api:
  # Server settings
  host: "0.0.0.0"
  port: 8000
  workers: 1
  
  # Interface settings
  interfaces:
    higress:
      enabled: true
      endpoint: "/api/v1"
    gradio:
      enabled: true
      port: 7860
      share: false
    comfyui:
      enabled: false
      port: 8188

# Cache settings
cache:
  # KV Cache (for AR models)
  kv_cache:
    enabled: true
    max_size: "2GB"
    
  # Diffusion Cache
  diffusion_cache:
    enabled: true
    max_size: "1GB"
    cache_latents: true

# Logging settings
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: null  # Set to file path for file logging
  
# Monitoring settings
monitoring:
  enabled: true
  metrics_port: 9090
  health_check_interval: 30  # seconds
